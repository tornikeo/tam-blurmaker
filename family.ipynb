{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woo hoo. Let's go!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "from cli import *\n",
    "\n",
    "print(\"Woo hoo. Let's go!\")\n",
    "\n",
    "# args, defined in track_anything.py\n",
    "# args = parse_argument()\n",
    "# args = default_args()\n",
    "args = argparse.Namespace()\n",
    "args.input = Path(\"test_sample/family_480.mp4\")\n",
    "args.track_data = Path(\"test_sample/family_480/sample_track_person.json\")\n",
    "args.device = \"cpu\"\n",
    "args.sam_model_type = \"vit_b\"\n",
    "args.output = Path(\"output.json\")\n",
    "args.debug = False\n",
    "args.mask_save = False\n",
    "args.output_video = Path(\"result.mp4\")\n",
    "args.track_data = json.load(open(args.track_data, \"r\"))\n",
    "# return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': {'frame': 200},\n",
       " 'points': [{'frame': 0, 'pos': [180, 176], 'label': 1}],\n",
       " 'end': {'frame': 300}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BaseSegmenter to cpu\n",
      "Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n",
      "Single object mode: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check and download checkpoints if needed\n",
    "SAM_checkpoint_dict = {\n",
    "    \"vit_h\": \"sam_vit_h_4b8939.pth\",\n",
    "    \"vit_l\": \"sam_vit_l_0b3195.pth\",\n",
    "    \"vit_b\": \"sam_vit_b_01ec64.pth\",\n",
    "}\n",
    "SAM_checkpoint_url_dict = {\n",
    "    \"vit_h\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
    "    \"vit_l\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n",
    "    \"vit_b\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n",
    "}\n",
    "sam_checkpoint = SAM_checkpoint_dict[args.sam_model_type]\n",
    "sam_checkpoint_url = SAM_checkpoint_url_dict[args.sam_model_type]\n",
    "xmem_checkpoint = \"XMem-s012.pth\"\n",
    "xmem_checkpoint_url = (\n",
    "    \"https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem-s012.pth\"\n",
    ")\n",
    "e2fgvi_checkpoint = \"E2FGVI-HQ-CVPR22.pth\"\n",
    "e2fgvi_checkpoint_id = \"10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3\"\n",
    "\n",
    "folder = \"./checkpoints\"\n",
    "SAM_checkpoint = download_checkpoint(sam_checkpoint_url, folder, sam_checkpoint)\n",
    "xmem_checkpoint = download_checkpoint(xmem_checkpoint_url, folder, xmem_checkpoint)\n",
    "e2fgvi_checkpoint = download_checkpoint_from_google_drive(\n",
    "    e2fgvi_checkpoint_id, folder, e2fgvi_checkpoint\n",
    ")\n",
    "# args.port = 12212\n",
    "# args.device = \"cuda:1\"\n",
    "# args.mask_save = True\n",
    "\n",
    "# initialize sam, xmem, e2fgvi models\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None, args)\n",
    "# video_input: /tmp/182f5d11c044d7004053ecf4b9f0678894a151ab/mall_480.mp4\n",
    "# video_state: {'user_name': '', 'video_name': '', 'origin_images': None, 'painted_images': None, 'masks': None, 'inpaint_masks': None, 'logits': None, 'select_frame_number': 0, 'fps': 30}\n",
    "interactive_state = {\n",
    "    \"inference_times\": 0,\n",
    "    \"negative_click_times\": 0,\n",
    "    \"positive_click_times\": 0,\n",
    "    \"mask_save\": args.mask_save,\n",
    "    \"multi_mask\": {\"mask_names\": [], \"masks\": []},\n",
    "    \"track_end_number\": args.track_data[\"end\"]['frame'],\n",
    "    \"resize_ratio\": 1,\n",
    "}\n",
    "video_state = {\n",
    "    \"track_start_number\": args.track_data[\"start\"]['frame'],\n",
    "    \"user_name\": \"\",\n",
    "    \"video_name\": \"\",\n",
    "    \"origin_images\": None,\n",
    "    \"painted_images\": None,\n",
    "    \"masks\": None,\n",
    "    \"inpaint_masks\": None,\n",
    "    \"logits\": None,\n",
    "    \"select_frame_number\": 0,\n",
    "    \"fps\": 30,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_state, video_info, origin_image = get_frames_from_video(\n",
    "    model,\n",
    "    args.input,\n",
    "    video_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "points = args.track_data['points']\n",
    "\n",
    "template_frame, video_state, interactive_state, run_status=select_template(\n",
    "    model,\n",
    "    points[0]['frame'], \n",
    "    video_state, \n",
    "    interactive_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evt = argparse.Namespace()\n",
    "evt.index = [0, 0]\n",
    "template_frame, video_state, interactive_state, run_status = sam_refine(\n",
    "    model=model,\n",
    "    video_state=video_state,\n",
    "    # point_prompt=sam_refine_args['point_prompt'],\n",
    "    point_prompt=None,#\"Positive\",\n",
    "    click_state=None,#[[180,176],[1]],\n",
    "    prompt={\n",
    "        \"prompt_type\": [\"click\"],\n",
    "        \"input_point\": [points[0]['pos']],#[[180,176]],\n",
    "        \"input_label\": [points[0]['label']],\n",
    "        \"multimask_output\": \"False\",\n",
    "    },\n",
    "    interactive_state=interactive_state,\n",
    "    evt=evt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image: 0it [00:00, ?it/s]\n",
      "/home/tornikeo/gcloud/testing/tam-blurmaker/cli.py:541: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  frames = torch.from_numpy(np.asarray(frames))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (717,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m video_output, video_state, interactive_state, run_status \u001b[39m=\u001b[39m vos_tracking_video(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      3\u001b[0m     video_output\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_video,\n\u001b[1;32m      4\u001b[0m     video_state\u001b[39m=\u001b[39;49mvideo_state,\n\u001b[1;32m      5\u001b[0m     interactive_state\u001b[39m=\u001b[39;49minteractive_state,\n\u001b[1;32m      6\u001b[0m     mask_dropdown\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m outputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/gcloud/testing/tam-blurmaker/cli.py:430\u001b[0m, in \u001b[0;36mvos_tracking_video\u001b[0;34m(model, video_state, interactive_state, mask_dropdown, video_output)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m video_output \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     video_output \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./result/track/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(video_state[\u001b[39m\"\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 430\u001b[0m video_output \u001b[39m=\u001b[39m generate_video_from_frames(\n\u001b[1;32m    431\u001b[0m     video_state[\u001b[39m\"\u001b[39;49m\u001b[39mpainted_images\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    432\u001b[0m     output_path\u001b[39m=\u001b[39;49mvideo_output,\n\u001b[1;32m    433\u001b[0m     fps\u001b[39m=\u001b[39;49mfps,\n\u001b[1;32m    434\u001b[0m )  \u001b[39m# import video_input to name the output video\u001b[39;00m\n\u001b[1;32m    435\u001b[0m interactive_state[\u001b[39m\"\u001b[39m\u001b[39minference_times\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m    438\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFor generating this tracking result, inference times: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, click times: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, positive: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, negative: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    439\u001b[0m         interactive_state[\u001b[39m\"\u001b[39m\u001b[39minference_times\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m )\n",
      "File \u001b[0;32m~/gcloud/testing/tam-blurmaker/cli.py:541\u001b[0m, in \u001b[0;36mgenerate_video_from_frames\u001b[0;34m(frames, output_path, fps)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39mGenerates a video from a list of frames.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39m    fps (int, optional): The frame rate of the output video. Defaults to 30.\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39m# height, width, layers = frames[0].shape\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m# fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39m# video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m \n\u001b[1;32m    540\u001b[0m \u001b[39m# video.release()\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m frames \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49masarray(frames))\n\u001b[1;32m    542\u001b[0m \u001b[39m# if not os.path.exists(os.path.dirname(output_path)):\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m#     os.makedirs(os.path.dirname(output_path))\u001b[39;00m\n\u001b[1;32m    544\u001b[0m torchvision\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mwrite_video(\u001b[39mstr\u001b[39m(output_path), frames, fps\u001b[39m=\u001b[39mfps, video_codec\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlibx264\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (717,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "video_output, video_state, interactive_state, run_status = vos_tracking_video(\n",
    "    model=model,\n",
    "    video_output=args.output_video,\n",
    "    video_state=video_state,\n",
    "    interactive_state=interactive_state,\n",
    "    mask_dropdown=[],\n",
    ")\n",
    "outputs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bbox2(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    try:\n",
    "        rmin, rmax = np.where(rows)[0][[0, -1]].tolist()\n",
    "        cmin, cmax = np.where(cols)[0][[0, -1]].tolist()\n",
    "        return rmin, rmax, cmin, cmax\n",
    "    except IndexError:\n",
    "        return None\n",
    "# video_state[\"masks\"][video_state[\"select_frame_number\"]] = mask\n",
    "for frame_num, mask in enumerate(video_state[\"masks\"]):\n",
    "    # print(mask)\n",
    "    # print(i)\n",
    "    # mask = np.load(mask)\n",
    "    # Get bounding box [x,y,x,y] from binary mask\n",
    "    bbox = bbox2(mask > 0)\n",
    "    if bbox is not None:\n",
    "        # Write outputs in {1: {'class': 0, 'bbox': [0, 0, 0, 0], 'score': ''}} format\n",
    "        outputs.append({frame_num: {'class': 0, 'bbox': bbox, 'score': ''}})\n",
    "\n",
    "# Write outputs to json\n",
    "Path(args.output).open('w').write(json.dumps({\n",
    "    'results': outputs\n",
    "}, indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
